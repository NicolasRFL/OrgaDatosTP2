{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Roberta.ipynb","provenance":[],"authorship_tag":"ABX9TyN8Z1JCtDb2G3zmnxzNoME3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"0mdBBVZiPHVr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597263904655,"user_tz":180,"elapsed":1496,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"c2a40999-e577-4de6-b8e5-e0b177a3cff9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vkYMGB-QPIu5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597263904656,"user_tz":180,"elapsed":1488,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"97fdb930-2d18-4377-ae6e-1820c2bb7c07"},"source":["%cd /content/gdrive/My Drive/Kaggle"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Kaggle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R7wYAPLYPKTa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263904656,"user_tz":180,"elapsed":1482,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCvnv-uvps_Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263907701,"user_tz":180,"elapsed":4523,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["from transformers import *\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgI1hofQPQ7B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1597263908323,"user_tz":180,"elapsed":5133,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"aa7358d9-8127-4a7e-c5ec-10c73bf649a7"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger') \n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential,Model\n","from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Flatten\n","from keras.layers import GlobalMaxPooling1D,Conv1D,LSTM\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","import tensorflow as tf\n","tf.python.control_flow_ops = tf"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Umzt217r0yFf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1597263912884,"user_tz":180,"elapsed":9684,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"61c0d6a4-5ffe-4609-ceed-bc5a4dc68ec2"},"source":["!pip install transformers\n","from transformers import RobertaTokenizer\n","from transformers import BertTokenizer\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LcNYYgAGPOYB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263912884,"user_tz":180,"elapsed":9679,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["train=pd.read_csv('train.csv',encoding='latin-1')\n","datos=pd.read_csv('test.csv',encoding='latin-1')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbQk2a5KpyBH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263913284,"user_tz":180,"elapsed":10074,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["bert_tokenizer_transformer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7mGjN42p0ud","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263913285,"user_tz":180,"elapsed":10071,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["def _get_segments(sentences):\n","    sentences_segments = []\n","    for sent in sentences:\n","      temp = []\n","      i = 0\n","      for token in sent.split(\" \"):\n","        temp.append(i)\n","        if token == \"[SEP]\":\n","          i += 1\n","      sentences_segments.append(temp)\n","    return sentences_segments\n","\n","def reempTexto(x):\n","    x=x.casefold()\n","    x=re.sub(r'http\\S*','http:',x)\n","    x=re.sub(r'[^a-z\\s]', '',x)\n","    return x\n","\n","def _get_inputs(df,_maxlen,tokenizer,use_keras_pad=False):\n","\n","\n","    sentences = [\"[CLS] \" + \" \".join(tokenizer.tokenize(y)) +\" [SEP] \" for y in (df['text'].apply(lambda x:reempTexto(x)).values.tolist())]\n","    \n","\n","    #generate masks\n","    # bert requires a mask for the words which are padded. \n","    # Say for example, maxlen is 100, sentence size is 90. then, [1]*90 + [0]*[100-90]\n","    sentences_mask = [[1]*len(sent.split(\" \"))+[0]*(_maxlen - len(sent.split(\" \"))) for sent in sentences]\n"," \n","    #generate input ids  \n","    # if less than max length provided then the words are padded\n","    if use_keras_pad:\n","      sentences_padded = pad_sequences(sentences.split(\" \"), dtype=object, maxlen=10, value='[PAD]',padding='post')\n","    else:\n","      sentences_padded = [sent + \" [PAD]\"*(_maxlen-len(sent.split(\" \"))) if len(sent.split(\" \"))!=_maxlen else sent for sent in sentences ]\n","\n","    sentences_converted = [tokenizer.convert_tokens_to_ids(s.split(\" \")) for s in sentences_padded]\n","    \n","    #generate segments\n","    # for each separation [SEP], a new segment is converted\n","    sentences_segment = _get_segments(sentences_padded)\n","\n","    genLength = set([len(sent.split(\" \")) for sent in sentences_padded])\n","\n","    if _maxlen < 20:\n","      raise Exception(\"max length cannot be less than 20\")\n","    elif len(genLength)!=1: \n","      print(genLength)\n","      raise Exception(\"sentences are not of same size\")\n","\n","\n","\n","    #convert list into tensor integer arrays and return it\n","    #return sentences_converted,sentences_segment, sentences_mask\n","    #return [np.asarray(sentences_converted, dtype=np.int32), \n","    #        np.asarray(sentences_segment, dtype=np.int32), \n","    #        np.asarray(sentences_mask, dtype=np.int32)]\n","    return [tf.cast(sentences_converted,tf.int32), tf.cast(sentences_segment,tf.int32), tf.cast(sentences_mask,tf.int32)]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qKJmXbWp87N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263919141,"user_tz":180,"elapsed":15923,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["bert_inputs = _get_inputs(df=train,tokenizer=bert_tokenizer_transformer,_maxlen=200)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"kKmOinGyp_Od","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263919145,"user_tz":180,"elapsed":15921,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["from tensorflow.keras.layers import Dense, Dropout,Embedding, LSTM, Bidirectional, Input, Dropout, GlobalAveragePooling1D\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import sequence"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zcd9--voqYbM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597263919146,"user_tz":180,"elapsed":15916,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"260970ac-8875-4f8f-aca9-c0278635bb7b"},"source":["Xtr = bert_inputs\n","ytr = np.asarray(train['target'].values)\n","\n","\"\"\"Xte = _get_inputs(test.head(),_maxlen=100, tokenizer = bert_tokenizer_transformer )\n","yte = np.asarray(test.iloc[:5,-30:])\"\"\""],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Xte = _get_inputs(test.head(),_maxlen=100, tokenizer = bert_tokenizer_transformer )\\nyte = np.asarray(test.iloc[:5,-30:])'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"7tZHCeSIqDtl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":530},"executionInfo":{"status":"ok","timestamp":1597263926213,"user_tz":180,"elapsed":22976,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"5d007930-7e56-4107-da58-fa2072e1e206"},"source":["#reference: https://github.com/huggingface/transformers/issues/1350\n","MAX_SEQUENCE_LENGTH = 200\n","\n","token_inputs = Input((MAX_SEQUENCE_LENGTH), dtype=tf.int32, name='input_word_ids')\n","mask_inputs = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n","seg_inputs = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n","\n","bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","seq_output,_ = bert_model([token_inputs, mask_inputs, seg_inputs])\n","X = GlobalAveragePooling1D()(seq_output)\n","X = Dense(100, activation='relu')(X)\n","output_= Dense(1, activation='sigmoid', name='output')(X)\n","\n","bert_model2 = Model([token_inputs, mask_inputs, seg_inputs],output_)\n","bert_model2.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 200)]        0                                            \n","__________________________________________________________________________________________________\n","input_masks (InputLayer)        [(None, 200)]        0                                            \n","__________________________________________________________________________________________________\n","input_segments (InputLayer)     [(None, 200)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model (TFBertModel)     ((None, 200, 768), ( 109482240   input_word_ids[0][0]             \n","                                                                 input_masks[0][0]                \n","                                                                 input_segments[0][0]             \n","__________________________________________________________________________________________________\n","global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 100)          76900       global_average_pooling1d[0][0]   \n","__________________________________________________________________________________________________\n","output (Dense)                  (None, 1)            101         dense[0][0]                      \n","==================================================================================================\n","Total params: 109,559,241\n","Trainable params: 109,559,241\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wrop3zPkqMaF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597263926214,"user_tz":180,"elapsed":22971,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["bert_model2.compile(optimizer='adam', loss='binary_crossentropy')"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"99VVXcCeqOF9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1597279992231,"user_tz":180,"elapsed":16088980,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"73a0b5ec-bd22-461f-e1d8-8d601fa6fd19"},"source":["bert_model2.fit(Xtr,ytr,epochs=1,batch_size = 20)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","381/381 [==============================] - 16006s 42s/step - loss: 0.7068\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f543f0eb400>"]},"metadata":{"tags":[]},"execution_count":15}]}]}