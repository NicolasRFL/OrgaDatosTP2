{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SuperRobertajin.ipynb","provenance":[],"authorship_tag":"ABX9TyMd886soOPTDQkVASDtLBwh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VS5_PCaoIuM3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1597303638857,"user_tz":180,"elapsed":24001,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"dae36dc2-beeb-4edf-d34a-9be017b26775"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiZjHxCiI1Xd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597303638860,"user_tz":180,"elapsed":23999,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"187b856c-5112-497d-fcca-6c3f59c6654e"},"source":["%cd /content/gdrive/My Drive/Kaggle"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Kaggle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9RyHHdsRI1Pt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597303638860,"user_tz":180,"elapsed":23995,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VRLrOBW7rH9C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1597303782158,"user_tz":180,"elapsed":6737,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"d59abbb1-797d-4b1e-af19-1a58709467cf"},"source":["!pip install transformers\n","import pandas as pd\n","# Recommended tensorflow version is <= 2.1.0, otherwise F1 score function breaks\n","import tensorflow as tf\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential,Model\n","from keras.layers import Dense, Activation, Flatten,concatenate\n","import tensorflow_datasets as tfds\n","from transformers import TFRobertaForSequenceClassification\n","from transformers import RobertaTokenizer\n","import os\n","import re"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiNBrjhyrKBw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597303784720,"user_tz":180,"elapsed":2558,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["train_def=pd.read_csv('dftrain')\n","datos_def=pd.read_csv('dfdatos')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"308o1i6HIdUH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"status":"error","timestamp":1597304504514,"user_tz":180,"elapsed":18428,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}},"outputId":"9e34506a-4f3e-4367-a00f-20e76a5dbc74"},"source":["\n","\n","def remTexto(x):\n","  x=x.casefold()\n","  x=re.sub(r'http\\S*', 'http:',x)\n","  x=re.sub(r'[^a-z\\s]', '',x)\n","  return x\n","\n","# Load your Dataset\n","train_tweets = pd.read_csv('train.csv')\n","test_tweets = pd.read_csv('test.csv')\n","\n","train_tweets['text']=train_tweets['text'].apply(lambda x:remTexto(x))\n","test_tweets['text']=test_tweets['text'].apply(lambda x:remTexto(x))\n","\n","training_sentences, testing_sentences = train_test_split(train_tweets[['text', 'target']],\n","                                                         test_size=0.2)\n","\n","roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","# can be up to 512 for BERT\n","max_length = 150\n","\n","# the recommended batches size for BERT are 32,64 ... however on this dataset we are overfitting quite fast\n","# and smaller batches work like a regularization.\n","\n","batch_size = 32\n","\n","def convert_example_to_feature(text):\n","    return roberta_tokenizer.encode_plus(text,\n","                                 add_special_tokens=True,  # add [CLS], [SEP]\n","                                 max_length=max_length,  # max length of the text that can go to RoBERTa\n","                                 pad_to_max_length=True,  # add [PAD] tokens at the end of sentence\n","                                 return_attention_mask=True,  # add attention mask to not focus on pad tokens\n","                                 truncation=True)\n","\n","# map to the expected input to TFRobertaForSequenceClassification, see here\n","def map_example_to_dict(input_ids, attention_masks, label):\n","    return {\n","      \"input_ids\": input_ids,\n","      \"attention_mask\": attention_masks,\n","           }, label\n","\n","def encode_examples(ds, limit=-1):\n","    # Prepare Input list\n","    input_ids_list = []\n","    attention_mask_list = []\n","    label_list = []\n","\n","    if (limit > 0):\n","        ds = ds.take(limit)\n","\n","    for review, label in tfds.as_numpy(ds):\n","        bert_input = convert_example_to_feature(review.decode())\n","        input_ids_list.append(bert_input['input_ids'])\n","        attention_mask_list.append(bert_input['attention_mask'])\n","        label_list.append([label])\n","\n","    return tf.data.Dataset.from_tensor_slices((input_ids_list,\n","                                               attention_mask_list,\n","                                               label_list)).map(map_example_to_dict)\n","\n","training_sentences_modified = tf.data.Dataset.from_tensor_slices((training_sentences['text'],\n","                                                                  training_sentences['target']))\n","\n","testing_sentences_modified = tf.data.Dataset.from_tensor_slices((testing_sentences['text'],\n","                                                                 testing_sentences['target']))\n","\n","ds_train_encoded = encode_examples(training_sentences_modified).shuffle(10000).batch(batch_size)\n","ds_test_encoded = encode_examples(testing_sentences_modified).batch(batch_size)\n","\n","\n","\n","learning_rate = 7e-5\n","number_of_epochs = 5\n","\n","\n","# model initialization\n","model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n","\n","NN_model = Sequential()\n","\n","# The Input Layer :\n","NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train_def.shape[1], activation='relu'))\n","NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n","NN_model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n","\n","# Compile the network :\n","\n","model_concat = concatenate([model.output, NN_model.output], axis=-1)\n","model_concat = Dense(1, activation='softmax')(model_concat)\n","model2 = Model(inputs=[model.input, NN_model.input], outputs=model_concat)\n","\n","model2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1abc30ab2981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Compile the network :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mmodel_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mmodel_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2103\u001b[0m     \"\"\"\n\u001b[1;32m   2104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2105\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Layer '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' has no inbound nodes.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output_tensors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Layer tf_roberta_for_sequence_classification_2 has no inbound nodes."]}]},{"cell_type":"code","metadata":{"id":"pXQQpXcHsXJp","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703563,"user_tz":180,"elapsed":88674,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["history=model.fit([ds_train_encoded,train_def], epochs=number_of_epochs,\n","          validation_data=ds_test_encoded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdT7zWRnUV5f","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703564,"user_tz":180,"elapsed":88671,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["import matplotlib.pyplot as plt\n","\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train','test'], loc = 'upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L331c52eKJ2_","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703564,"user_tz":180,"elapsed":88668,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["#Tengo que agregar esto por como esta definido tensor slices del dataset original\n","#Luego sera ignorado por el modelo\n","test_tweets['tar']=0\n","submission_sentences_modified = tf.data.Dataset.from_tensor_slices((test_tweets['text'],test_tweets['tar']))\n","test_encoded = encode_examples(submission_sentences_modified).batch(batch_size)\n","x=model.predict([test_encoded,test_def])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e7pZvpqN96f","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703565,"user_tz":180,"elapsed":88666,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["submission_pre = tf.nn.softmax(x[0])\n","submission_pre_argmax = tf.math.argmax(submission_pre, axis=1)\n","submission_pre_argmax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7bhjk0fLhxO","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703565,"user_tz":180,"elapsed":88662,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["res=pd.DataFrame()\n","res['target']=submission_pre_argmax.numpy()\n","res['id']=test_tweets['id']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oft7xcBPJjX","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703566,"user_tz":180,"elapsed":88660,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["res.to_csv('resultadosRoberta.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBQGVop3PnoV","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597303703566,"user_tz":180,"elapsed":88657,"user":{"displayName":"Niko Farfan","photoUrl":"","userId":"18312784935668397664"}}},"source":["#!kaggle competitions submit -c nlp-getting-started -f resultadosRoberta.csv -m \"Nuevo intento\""],"execution_count":null,"outputs":[]}]}