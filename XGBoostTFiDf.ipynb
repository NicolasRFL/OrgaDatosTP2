{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BspM5y3MVct"
   },
   "source": [
    "Parallel tree boosting. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bvNYHf8bq5BK",
    "outputId": "7d225307-9512-438f-c493-a4719d5a763b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/gdrive')\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/gdrive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1uBSFc01secX",
    "outputId": "f659936b-574f-4f78-94d1-9936d3cad0e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%cd /content/gdrive/My Drive/Kaggle'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%cd /content/gdrive/My Drive/Kaggle\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSnC7GHFrJVw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nos.environ[\\'KAGGLE_CONFIG_DIR\\'] = \"/content/gdrive/My Drive/Kaggle'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "iaW-ELSQ6ewH",
    "outputId": "51977855-df82-4544-b175-44bd6db6561d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nicolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nicolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nicolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nicolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U571Bj9aaKnk"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f94b15713472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloadtxt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "caU5q_PoaO1N",
    "outputId": "9de2321a-289b-4619-e874-d1a70f31b822",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "El6KSFBojdtp",
    "outputId": "49593e6b-00de-4eb4-ebaf-2d288f945933"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "O0aqB9EerTjg",
    "outputId": "5fb528aa-6546-48a6-aef6-c13bc6251c02"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "KEtYB2Juremn",
    "outputId": "4c7f1e10-5fa0-4792-961f-4dedf68803f4"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\",encoding='latin-1')\n",
    "datos =pd.read_csv(\"test.csv\",encoding='latin-1')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N475kllAI4ib"
   },
   "outputs": [],
   "source": [
    "serie=datos['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6-ZQ5gyxVbf"
   },
   "outputs": [],
   "source": [
    "#Separo el target de los tweets\n",
    "target = train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xM-jumC08FM-",
    "outputId": "be342d04-8940-43a9-d2d1-9c0de292738b"
   },
   "outputs": [],
   "source": [
    "#Aplico operaciones sobre train que luego replicare en el set de datos, para obtener los indices que yo considero relevantes\n",
    "#y deshacerme de los que no lo son\n",
    "#Empiezo por las arroba\n",
    "train['cantidad@']=train['text'].str.count('@')\n",
    "datos['cantidad@']=datos['text'].str.count('@')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IamIJ8qy8Fu7"
   },
   "outputs": [],
   "source": [
    "#Aplico una funcion de hash sobre la location\n",
    "#Primero fijo los NaN como la cadena \"ninguna\"\n",
    "#Luego proceso eliminando mayusculas haciendo lemmatize\n",
    "lemma=WordNetLemmatizer()\n",
    "def reemplazarTexto(x):\n",
    "  x=x.casefold()\n",
    "  x=lemma.lemmatize(x)\n",
    "  return x\n",
    "\n",
    "def procesarLocation(df):\n",
    "  df['location']=df['location'].fillna(value='none')\n",
    "  return df['location'].apply(lambda x:reemplazarTexto(x))\n",
    "train['location']=procesarLocation(train)\n",
    "datos['location']=procesarLocation(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LVx6GPraxqRM",
    "outputId": "cd74d95d-06df-460d-bf97-a2172d9d2849"
   },
   "outputs": [],
   "source": [
    "def crearFeatureHasher(serie,ndf):\n",
    "  fh = FeatureHasher(n_features=6, input_type='string')\n",
    "  sp = fh.fit_transform(serie)\n",
    "  df = pd.DataFrame(sp.toarray(), columns=['fh1', 'fh2', 'fh3', 'fh4', 'fh5', 'fh6'])\n",
    "  ndf=pd.concat([ndf, df], axis=1)\n",
    "  return ndf\n",
    "train=crearFeatureHasher(train['location'],train)\n",
    "datos=crearFeatureHasher(datos['location'],datos)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2tVKI81Y-DXV",
    "outputId": "0144ad29-b281-47bd-d0fe-0db099284e53"
   },
   "outputs": [],
   "source": [
    "#VERIFICAR QUE LOS LOCATION SEAN DE UBICACIONES VERDADERAS\n",
    "paises=['afghanistan', 'albania', 'algeria', 'andorra', 'angola', 'antigua & barbuda', 'argentina', 'armenia', 'australia', 'austria','azerbaijan', 'bahamas', 'bahrain','bangladesh','barbados', 'belarus', 'belgium', 'belize', 'benin', 'bhutan', 'bolivia', \n",
    "       'bosnia & herzegovina', 'botswana', 'brazil', 'bulgaria', 'burkina faso', 'burundi', 'cabo verde', 'cambodia', 'cameroon', \n",
    "       'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa rica', 'croatia', 'cuba', 'cyprus', 'czech republic', \n",
    "       'denmark', 'djibouti', 'dominica', 'dominican republic', 'ecuador', 'egypt', 'el salvador', 'equatorial guinea', 'eritrea', 'estonia', \n",
    "       'eswatini', 'fiji', 'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'greece', 'grenada', 'guatemala', \n",
    "       'guinea', 'guinea-bissau', 'guyana', 'haiti', 'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', \n",
    "       'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'kosovo', 'kuwait', \n",
    "       'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania', 'luxembourg', \n",
    "       'madagascar', 'malawi', 'malaysia kuala lumpur', 'maldives male', 'mali', 'malta', 'marshall', 'islands', 'mauritania', 'mauritius', \n",
    "       'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'namibia', 'nepal', \n",
    "       'netherlands', 'new zealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'pakistan', \n",
    "       'palestine', 'panama', 'papua new guinea', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', \n",
    "       'russia moscow', 'rwanda kigali', 'saudi arabia', 'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia','south africa', 'south korea', \n",
    "       'south sudan juba', 'spain', 'sri lanka', 'sudan', 'suriname', 'sweden ', 'switzerland bern', 'syria', 'tajikistan','thailand', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'united arab emirates', 'uae',\n",
    "       'united kingdom', 'uk', 'united states of america', 'usa', 'uruguay', 'uzbekistan', \n",
    "       'vatican city', 'holy see', 'venezuela',  'vietnam', 'yemen', 'zambia','zimbabwe']\n",
    "capitales=['kabul','tirana','algiers','andorra la vella','luanda','buenos aires','yerevan','canberra','vienna','baku','manama','dhaka','minsk','bridgetown','brussels',\n",
    "           'belmopan','porto-novo','thimphu','sucre','sarajevo','gaborone','brasilia','sofia','bujumbura','praia','phnom penh','yaounde','ottawa','santiago','beijing','bogotá',' moroni','san jose',\n",
    "           ' zagreb','havana','nicosia','prague','copenhagen','roseau','santo domingo','quito','cairo','san salvador','malabo','asmara','tallinn','mbabane','suva','helsinki','paris','libreville','tbilisi','berlin',\n",
    "           'accra','athens','guatemala city','mexico city','mexico df','conakry','bissau','georgetown','port-au-prince','tegucigalpa','budapest','reykjavik','new delhi','jakarta','tehran','baghdad','dublin','jerusalem',\n",
    "           'rome','kingston','tokyo','amman','nairobi','pristina','kuwait city','bishkek','vientiane','riga','beirut','maseru','monrovia','tripoli','vaduz','vilnius','luxembourg','antananarivo','lilongwe',\n",
    "           'monaco','podgorica','bamako','valletta','majuro','nouakchott','windhoek','rabat','maputo','kathmandu','port louis','amsterdam','wellington','managua','niamey','abuja','colombo','belgrade',\n",
    "           'panama city','east jerusalem','port moresby','asunción','lima','madrid','khartoum','paramaribo','stockholm','freetown','singapore','caracas','damascus','dushanbe','harare','bangkok',\n",
    "           'ankara','bucharest','riyadh','manila','warsaw','oslo','islamabad','lisbon','victoria','bratislava','seoul','kiev','london','abu dhabi','washington','washington dc','washington d.c.', 'd.c.','montevideo','tashkent', 'vatican city',\n",
    "           'hanoi']\n",
    "estadosUS= [\"alabama\",\"alaska\",\"arizona\",\"arkansas\",\"california\",\"colorado\",\\\n",
    "  \"connecticut\",\"delaware\",\"florida\",\"georgia\",\"hawaii\",\"idaho\",\"illinois\",\\\n",
    "  \"indiana\",\"iowa\",\"kansas\",\"kentucky\",\"louisiana\",\"maine\",\"maryland\",\\\n",
    "  \"massachusetts\",\"michigan\",\"minnesota\",\"mississippi\",\"missouri\",\"montana\",\\\n",
    "  \"nebraska\",\"nevada\",\"new hampshire\",\"new jersey\",\"new mexico\",\"new york\",\\\n",
    "  \"north carolina\",\"north dakota\",\"ohio\",\"oklahoma\",\"oregon\",\"pennsylvania\",\\\n",
    "  \"rhode island\",\"south carolina\",\"south dakota\",\"tennessee\",\"texas\",\"utah\",\\\n",
    "  \"vermont\",\"virginia\",\"washington\",\"west virginia\",\"wisconsin\",\"wyoming\"]\n",
    "train['locationPais']=train['location'].str.contains('|'.join(paises))*1\n",
    "datos['locationPais']=datos['location'].str.contains('|'.join(paises))*1\n",
    "train['locationCiudad']=(train['location'].str.contains('|'.join(capitales)) | train['location'].str.contains('|'.join(estadosUS)))*1\n",
    "datos['locationCiudad']=(datos['location'].str.contains('|'.join(capitales)) | datos['location'].str.contains('|'.join(estadosUS)))*1\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmLZSbOW3mFu"
   },
   "outputs": [],
   "source": [
    "#Analizo longitud de los tweets\n",
    "#Convertir lo categorico en binario usando one hot encoding\n",
    "train['longitudTweet']=train.text.str.len()\n",
    "datos['longitudTweet']=datos.text.str.len()\n",
    "\n",
    "bins =[-np.inf,20, 40, 60,80, 100, 120, 140, 160, 180, 200, np.inf]\n",
    "l=['0-20l','20-40l','40-60l','60-80l','80-100l','100-120l','120-140l','140-160l','160-180l','180-200l','200+l']\n",
    "trainCatLong=pd.cut(train['longitudTweet'], bins,labels=l)\n",
    "datosCatLong=pd.cut(datos['longitudTweet'], bins,labels=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "9AcE5DaIglVS",
    "outputId": "11f5595e-beb0-4cdc-b3dc-23fb4af6959b"
   },
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "train = pd.concat([train,pd.get_dummies(trainCatLong)],axis=1)\n",
    "datos = pd.concat([datos,pd.get_dummies(datosCatLong)],axis=1)\n",
    "datos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJu6ioS24_LP"
   },
   "outputs": [],
   "source": [
    "#Analizo la cantidad de Hashtags en un tweet\n",
    "train['cantidad#']=train['text'].str.count('#')\n",
    "datos['cantidad#']=datos['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxpTtaAG401N"
   },
   "outputs": [],
   "source": [
    "#Hago binary encoding de la cantidad de hashtags en un tweet\n",
    "import math\n",
    "def convertBin(x,maximo):\n",
    "  lista=[]\n",
    "  binario=format(x,'02b')\n",
    "  while len(binario)<maximo:\n",
    "    binario='0'+binario\n",
    "  for i in binario:\n",
    "    lista.append(int(i))\n",
    "  return lista\n",
    "\n",
    "def binaryEncoding(serie,longMax):\n",
    "  longMax+=1\n",
    "  binaryHash=serie.apply(lambda x:convertBin(x,longMax))\n",
    "  ncol=[]\n",
    "  for i in range(longMax-1,-1,-1):\n",
    "    ncol.append(str(i)+'b'+serie.name)\n",
    "  return pd.DataFrame((item for item in binaryHash),columns=ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuFI5Nk2yCLn"
   },
   "outputs": [],
   "source": [
    "longMax=max(math.floor(math.log(train['cantidad#'].max(),2)),math.floor(math.log(datos['cantidad#'].max(),2)))\n",
    "#Calculo asi long max porque el train y test tienen que tener la misma cant de col\n",
    "train = pd.concat([train,binaryEncoding(train['cantidad#'],longMax)],axis=1)\n",
    "datos = pd.concat([datos,binaryEncoding(datos['cantidad#'],longMax)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "A2awrmTYIFn1",
    "outputId": "50d07da1-68ed-43b5-8abc-0ec3c9dc997f"
   },
   "outputs": [],
   "source": [
    "#Hago binary encoding de la cantidad de @ en los tweets\n",
    "longMax=max(math.floor(math.log(train['cantidad@'].max(),2)),math.floor(math.log(datos['cantidad@'].max(),2)))\n",
    "#Calculo asi long max porque el train y test tienen que tener la misma cant de col\n",
    "train = pd.concat([train,binaryEncoding(train['cantidad@'],longMax)],axis=1)\n",
    "datos = pd.concat([datos,binaryEncoding(datos['cantidad@'],longMax)],axis=1)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CnaEO3wmFun"
   },
   "outputs": [],
   "source": [
    "#Cuento si el texto contiene un http y cuantos tiene\n",
    "def analizarHttp(x):\n",
    "  tokens=nltk.word_tokenize(x)\n",
    "  resultado=0\n",
    "  for token in tokens:\n",
    "    resultado += (token=='http')*1      \n",
    "  return resultado\n",
    "\n",
    "train['cantidadLinks']=train['text'].apply(lambda x:analizarHttp(x))\n",
    "datos['cantidadLinks']=datos['text'].apply(lambda x:analizarHttp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "-KY-Vft8umQA",
    "outputId": "b4595296-a9ce-4634-95b3-ce4642a5abad"
   },
   "outputs": [],
   "source": [
    "#Hago binary encoding de https\n",
    "longMax=max(math.floor(math.log(train['cantidadLinks'].max(),2)),math.floor(math.log(datos['cantidadLinks'].max(),2)))\n",
    "#Calculo asi long max porque el train y test tienen que tener la misma cant de col\n",
    "train = pd.concat([train,binaryEncoding(train['cantidadLinks'],longMax)],axis=1)\n",
    "datos = pd.concat([datos,binaryEncoding(datos['cantidadLinks'],longMax)],axis=1)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6sHytRqZrtY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def reemplazarTexto(x):\n",
    "  x=x.casefold()\n",
    "  x=lemma.lemmatize(x)\n",
    "  x=x.casefold()\n",
    "  x=re.sub(r'http\\S*', 'http:',x)\n",
    "  x=re.sub(r'@[A-Za-z0-9]+','',x)\n",
    "  x=re.sub(r'[^a-z\\s]', '',x)\n",
    "  return x\n",
    "train['textoProcesado']=train['text'].apply(lambda x:reemplazarTexto(x))\n",
    "datos['textoProcesado']=datos['text'].apply(lambda x:reemplazarTexto(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DK-AlK8n_Gnm"
   },
   "outputs": [],
   "source": [
    "#Primero preproceso eliminando mayusculas haciendo lemmatize\n",
    "train['keyword'].fillna(value='ninguna',inplace=True)\n",
    "datos['keyword'].fillna(value='ninguna',inplace=True)\n",
    "lemma=WordNetLemmatizer()\n",
    "train['keyword']=train['keyword'].apply(lambda x:reemplazarTexto(x))\n",
    "datos['keyword']=datos['keyword'].apply(lambda x:reemplazarTexto(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwOSL4bluK9x"
   },
   "outputs": [],
   "source": [
    "#Analizo si los hashtags contienen alguna keyword\n",
    "def analizarHashtag(x,keys):\n",
    "  tokens=nltk.word_tokenize(x)\n",
    "  resultado=False\n",
    "  for i in range(len(tokens)):\n",
    "    if ((tokens[i]=='#') & (i!=len(tokens)-1)):\n",
    "      resultado = lemma.lemmatize(tokens[i+1]) in keys\n",
    "  return resultado*1\n",
    "lTrainK=train['keyword'].unique().tolist()\n",
    "lDatosK=train['keyword'].unique().tolist()\n",
    "train['hashtagKey'] = train['textoProcesado'].apply(lambda x:analizarHashtag(x,lTrainK))\n",
    "datos['hashtagKey'] = datos['textoProcesado'].apply(lambda x:analizarHashtag(x,lDatosK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhLTZmgx6Ai8"
   },
   "outputs": [],
   "source": [
    "#Analizo si se menciona algun tipo de entretenimiento en el tweet\n",
    "#Puede ser juegos, musica, peliculas...\n",
    "listaEntretenimiento=['music','radio','tv','show','dj','game','gaming','videogame','history','movies','youtube','book','ebook','sports','esports']\n",
    "\n",
    "def analizarContenido(x):\n",
    "  tokens=nltk.word_tokenize(x)\n",
    "  resultado=0\n",
    "  for palabra in tokens:\n",
    "    if palabra in listaEntretenimiento:\n",
    "      resultado = 1\n",
    "  return resultado\n",
    "\n",
    "train['entretenimiento'] = train['textoProcesado'].apply(lambda x:analizarContenido(x))\n",
    "datos['entretenimiento'] = datos['textoProcesado'].apply(lambda x:analizarContenido(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YuKha9oIhBi"
   },
   "outputs": [],
   "source": [
    "#Analizo cantidad de palabras en los tweets\n",
    "def contarPalabras (d):\n",
    "  return (len(str.split(d,' ')))\n",
    "train['cantidadPalabras']=train['text'].apply(lambda x: contarPalabras(x))\n",
    "datos['cantidadPalabras']=datos['text'].apply(lambda x: contarPalabras (x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "1fKSNgTf-0HT",
    "outputId": "a1a89107-8d43-4ff0-9831-d784030b52e7"
   },
   "outputs": [],
   "source": [
    "#Hago hashing trick para keywords\n",
    "def crearFeatureHasher(serie,ndf):\n",
    "  fh = FeatureHasher(n_features=6, input_type='string')\n",
    "  sp = fh.fit_transform(serie)\n",
    "  df = pd.DataFrame(sp.toarray(), columns=['kh1', 'kh2', 'kh3', 'kh4', 'kh5', 'kh6'])\n",
    "  ndf=pd.concat([ndf, df], axis=1)\n",
    "  return ndf\n",
    "train=crearFeatureHasher(train['keyword'],train)\n",
    "datos=crearFeatureHasher(datos['keyword'],datos)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgajeiQgmCPU"
   },
   "outputs": [],
   "source": [
    "#Analizo la relacion entre cantidad de # y la longitud del tweet\n",
    "#Hacer esto categorico\n",
    "train['relacion#longitud']=train['cantidad#']/train['cantidadPalabras']*100\n",
    "datos['relacion#longitud']=datos['cantidad#']/datos['cantidadPalabras']*100\n",
    "l=['pocos#','medio#','muchos#']\n",
    "trainCatLong=pd.cut(train['relacion#longitud'], 3,labels=l)\n",
    "datosCatLong=pd.cut(datos['relacion#longitud'], 3,labels=l)\n",
    "#One hot encoding\n",
    "train = pd.concat([train,pd.get_dummies(trainCatLong)],axis=1)\n",
    "datos = pd.concat([datos,pd.get_dummies(datosCatLong)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "m76PIzsznTVd",
    "outputId": "369773b9-c8f3-45bf-cbce-d6d8af1c92a4"
   },
   "outputs": [],
   "source": [
    "#Analizo la relacion entre cantidad de @ y la longitud del tweet\n",
    "#Hacer esto categorico\n",
    "train['relacion@longitud']=train['cantidad@']/train['cantidadPalabras']*100\n",
    "datos['relacion@longitud']=datos['cantidad@']/datos['cantidadPalabras']*100\n",
    "l=['pocos@','medio@','muchos@']\n",
    "trainCatLong=pd.cut(train['relacion@longitud'], 3,duplicates='drop',labels=l)\n",
    "datosCatLong=pd.cut(datos['relacion@longitud'], 3,duplicates='drop',labels=l)\n",
    "#One hot encoding\n",
    "train = pd.concat([train,pd.get_dummies(trainCatLong)],axis=1)\n",
    "datos = pd.concat([datos,pd.get_dummies(datosCatLong)],axis=1)\n",
    "datos.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXyamVDuyhMb"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(pd.concat([train,datos])['textoProcesado'].values.tolist())\n",
    "matTrain=vectorizer.transform(train['textoProcesado'].values.tolist())\n",
    "matDatos=vectorizer.transform(datos['textoProcesado'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tN9HJWhc47EM"
   },
   "outputs": [],
   "source": [
    "train=pd.concat([train,pd.DataFrame(matTrain.todense(),columns=vectorizer.get_feature_names())],axis=1)\n",
    "datos=pd.concat([datos,pd.DataFrame(matDatos.todense(),columns=vectorizer.get_feature_names())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZIMK4mcpFQV"
   },
   "outputs": [],
   "source": [
    "#Calculo la cantidad de stopwords por texto\n",
    "from nltk.corpus import stopwords\n",
    "def longitudSinStopword(x):\n",
    "  tokens=nltk.word_tokenize(x)\n",
    "  filtered_words = [word for word in tokens if word in stopwords.words('english')]\n",
    "  return len(filtered_words)\n",
    "datos['stopWordsXTexto']=pd.qcut(datos['textoProcesado'].apply(lambda x: longitudSinStopword(x))/datos['textoProcesado'].str.len(),q=5)\n",
    "train['stopWordsXTexto']=pd.qcut((train['textoProcesado'].apply(lambda x: longitudSinStopword(x)))/train['textoProcesado'].str.len(),q=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAffCAe6Hfy8"
   },
   "outputs": [],
   "source": [
    "#Remuevo cosas que no hacen al analisis\n",
    "entrenar=train.drop(columns=['target','id','keyword','location','text','textoProcesado','stopWordsXTexto','relacion#longitud','relacion@longitud','longitudTweet','cantidad@','cantidad#','cantidadPalabras'])\n",
    "test=datos.drop(columns=['target','id','keyword','location','text','textoProcesado','stopWordsXTexto','relacion#longitud','relacion@longitud','longitudTweet','cantidad@','cantidad#','cantidadPalabras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "4dY2-0wayohY",
    "outputId": "cb236635-7cce-41e2-f9ac-9e9637ac2e0f"
   },
   "outputs": [],
   "source": [
    "entrenar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MG8QmUTRISuL"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#entrenar.info()\n",
    "#X_train, X_test, y_train, y_test =  train_test_split(entrenar, target, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7haa4xCWx4s9"
   },
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                colsample_bytree = 0.4, learning_rate = 0.2,\n",
    "                max_depth = 6, alpha = 10, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ovG65_arE-0R",
    "outputId": "d085e03b-9ad1-4fa8-c5c8-9ee045ac4920"
   },
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X = entrenar\n",
    "Y = target\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "# fit model no training data\n",
    "model = xg_reg\n",
    "eval_set = [(X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=\"error\")\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CXODAHHda4gh",
    "outputId": "45c049a3-25e0-4075-b932-fb063e7d710b"
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "nmSoXon1FAo3",
    "outputId": "b990bda2-2f2e-4da3-a592-71b33686cd29"
   },
   "outputs": [],
   "source": [
    "#Nota target es una columna en test por el TF idf\n",
    "preds = xg_reg.predict(test)\n",
    "preds = (preds>=0.5)*1\n",
    "predictions = pd.DataFrame()\n",
    "predictions['id']=serie\n",
    "predictions['target']=preds\n",
    "predictions.head()\n",
    "predictions.to_csv('resultadosTFIDF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6E0NLliIw1Q"
   },
   "outputs": [],
   "source": [
    "#!kaggle competitions submit -c nlp-getting-started -f resultados.csv -m \"Nuevo intento\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XGBoostTFiDf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
