{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uBSFc01secX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6875d43a-f5bf-411a-9a83-e0ab6f77513e"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/My Drive/Kaggle'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvNYHf8bq5BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "347ba826-583e-4d91-9199-a68ad81c1522"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSnC7GHFrJVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U571Bj9aaKnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caU5q_PoaO1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El6KSFBojdtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0aqB9EerTjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEtYB2Juremn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv(\"train.csv\",encoding='latin-1')\n",
        "datos =pd.read_csv(\"test.csv\",encoding='latin-1')\n",
        "datos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6-ZQ5gyxVbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separo el target de los tweets\n",
        "target = train.iloc[:,-1]\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM-jumC08FM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplico operaciones sobre train que luego replicare en el set de datos, para obtener los indices que yo considero relevantes\n",
        "#y deshacerme de los que no lo son\n",
        "#Empiezo por las arroba\n",
        "train['cantidad@']=train['text'].str.count('@')\n",
        "datos['cantidad@']=datos['text'].str.count('@')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IamIJ8qy8Fu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplico una funcion de hash sobre la location\n",
        "#Primero fijo los NaN como la cadena \"ninguna\"\n",
        "train['location']=train['location'].fillna(value='ninguna')\n",
        "datos['location']=datos['location'].fillna(value='ninguna')\n",
        "train['location']=train['location'].str.casefold()\n",
        "datos['location']=datos['location'].str.casefold()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVx6GPraxqRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crearFeatureHasher(serie,ndf):\n",
        "  fh = FeatureHasher(n_features=6, input_type='string')\n",
        "  sp = fh.fit_transform(serie)\n",
        "  df = pd.DataFrame(sp.toarray(), columns=['fh1', 'fh2', 'fh3', 'fh4', 'fh5', 'fh6'])\n",
        "  ndf=pd.concat([ndf, df], axis=1)\n",
        "  return ndf\n",
        "train=crearFeatureHasher(train['location'],train)\n",
        "datos=crearFeatureHasher(datos['location'],datos)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tVKI81Y-DXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#VERIFICAR QUE LOS LOCATION SEAN DE UBICACIONES VERDADERAS\n",
        "paises=['afghanistan', 'albania', 'algeria', 'andorra', 'angola', 'antigua & barbuda', 'argentina', 'armenia', 'australia', 'austria','azerbaijan', 'bahamas', 'bahrain','bangladesh','barbados', 'belarus', 'belgium', 'belize', 'benin', 'bhutan', 'bolivia', \n",
        "       'bosnia & herzegovina', 'botswana', 'brazil', 'bulgaria', 'burkina faso', 'burundi', 'cabo verde', 'cambodia', 'cameroon', \n",
        "       'canada', 'chad', 'chile', 'china', 'colombia', 'comoros', 'congo', 'costa rica', 'croatia', 'cuba', 'cyprus', 'czech republic', \n",
        "       'denmark', 'djibouti', 'dominica', 'dominican republic', 'ecuador', 'egypt', 'el salvador', 'equatorial guinea', 'eritrea', 'estonia', \n",
        "       'eswatini', 'fiji', 'finland', 'france', 'gabon', 'gambia', 'georgia', 'germany', 'ghana', 'greece', 'grenada', 'guatemala', \n",
        "       'guinea', 'guinea-bissau', 'guyana', 'haiti', 'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', \n",
        "       'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'jordan', 'kazakhstan', 'kenya', 'kosovo', 'kuwait', \n",
        "       'kyrgyzstan', 'laos', 'latvia', 'lebanon', 'lesotho', 'liberia', 'libya', 'liechtenstein', 'lithuania', 'luxembourg', \n",
        "       'madagascar', 'malawi', 'malaysia kuala lumpur', 'maldives male', 'mali', 'malta', 'marshall', 'islands', 'mauritania', 'mauritius', \n",
        "       'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'myanmar', 'namibia', 'nepal', \n",
        "       'netherlands', 'new zealand', 'nicaragua', 'niger', 'nigeria', 'norway', 'pakistan', \n",
        "       'palestine', 'panama', 'papua new guinea', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', \n",
        "       'russia moscow', 'rwanda kigali', 'saudi arabia', 'serbia', 'seychelles', 'sierra leone', 'singapore', 'slovakia', 'slovenia','south africa', 'south korea', \n",
        "       'south sudan juba', 'spain', 'sri lanka', 'sudan', 'suriname', 'sweden ', 'switzerland bern', 'syria', 'tajikistan','thailand', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'united arab emirates', 'uae',\n",
        "       'united kingdom', 'uk', 'united states of america', 'usa', 'uruguay', 'uzbekistan', \n",
        "       'vatican city', 'holy see', 'venezuela',  'vietnam', 'yemen', 'zambia','zimbabwe']\n",
        "capitales=['kabul','tirana','algiers','andorra la vella','luanda','buenos aires','yerevan','canberra','vienna','baku','manama','dhaka','minsk','bridgetown','brussels',\n",
        "           'belmopan','porto-novo','thimphu','sucre','sarajevo','gaborone','brasilia','sofia','bujumbura','praia','phnom penh','yaounde','ottawa','santiago','beijing','bogotá',' moroni','san jose',\n",
        "           ' zagreb','havana','nicosia','prague','copenhagen','roseau','santo domingo','quito','cairo','san salvador','malabo','asmara','tallinn','mbabane','suva','helsinki','paris','libreville','tbilisi','berlin',\n",
        "           'accra','athens','guatemala city','mexico city','mexico df','conakry','bissau','georgetown','port-au-prince','tegucigalpa','budapest','reykjavik','new delhi','jakarta','tehran','baghdad','dublin','jerusalem',\n",
        "           'rome','kingston','tokyo','amman','nairobi','pristina','kuwait city','bishkek','vientiane','riga','beirut','maseru','monrovia','tripoli','vaduz','vilnius','luxembourg','antananarivo','lilongwe',\n",
        "           'monaco','podgorica','bamako','valletta','majuro','nouakchott','windhoek','rabat','maputo','kathmandu','port louis','amsterdam','wellington','managua','niamey','abuja','colombo','belgrade',\n",
        "           'panama city','east jerusalem','port moresby','asunción','lima','madrid','khartoum','paramaribo','stockholm','freetown','singapore','caracas','damascus','dushanbe','harare','bangkok',\n",
        "           'ankara','bucharest','riyadh','manila','warsaw','oslo','islamabad','lisbon','victoria','bratislava','seoul','kiev','london','abu dhabi','washington','washington dc','washington d.c.', 'd.c.','montevideo','tashkent', 'vatican city',\n",
        "           'hanoi']\n",
        "estadosUS= [\"alabama\",\"alaska\",\"arizona\",\"arkansas\",\"california\",\"colorado\",\\\n",
        "  \"connecticut\",\"delaware\",\"florida\",\"georgia\",\"hawaii\",\"idaho\",\"illinois\",\\\n",
        "  \"indiana\",\"iowa\",\"kansas\",\"kentucky\",\"louisiana\",\"maine\",\"maryland\",\\\n",
        "  \"massachusetts\",\"michigan\",\"minnesota\",\"mississippi\",\"missouri\",\"montana\",\\\n",
        "  \"nebraska\",\"nevada\",\"new hampshire\",\"new jersey\",\"new mexico\",\"new york\",\\\n",
        "  \"north carolina\",\"north dakota\",\"ohio\",\"oklahoma\",\"oregon\",\"pennsylvania\",\\\n",
        "  \"rhode island\",\"south carolina\",\"south dakota\",\"tennessee\",\"texas\",\"utah\",\\\n",
        "  \"vermont\",\"virginia\",\"washington\",\"west virginia\",\"wisconsin\",\"wyoming\"]\n",
        "train['locationPais']=train['location'].str.contains('|'.join(paises))*1\n",
        "datos['locationPais']=datos['location'].str.contains('|'.join(paises))*1\n",
        "train['locationCiudad']=(train['location'].str.contains('|'.join(capitales)) | train['location'].str.contains('|'.join(estadosUS)))*1\n",
        "datos['locationCiudad']=(datos['location'].str.contains('|'.join(capitales)) | datos['location'].str.contains('|'.join(estadosUS)))*1\n",
        "train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjC0Hg5qUcQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['locationPais'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmLZSbOW3mFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Analizo longitud de los tweets\n",
        "train['longitudTweet']=train.text.str.len()\n",
        "datos['longitudTweet']=datos.text.str.len()\n",
        "datos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJu6ioS24_LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Analizo la cantidad de Hashtags en un tweet\n",
        "train['cantidad#']=train['text'].str.count('#')\n",
        "datos['cantidad#']=datos['text'].str.count('#')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQn2mD6x5Kps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Analizo que contiene cada hashtag\n",
        "#creo un feature hashing para cada hashtag\n",
        "\"\"\"def crearFeatureHasher(serie,ndf):\n",
        "  fh = FeatureHasher(n_features=6, input_type='string')\n",
        "  sp = fh.fit_transform(serie)\n",
        "  df = pd.DataFrame(sp.toarray(), columns=['f1', 'f2', 'f3', 'f4', 'f5', 'f6','f7','f8','f9','f10','f11'])\n",
        "  ndf=pd.concat([ndf, df], axis=1)\n",
        "  return ndf\n",
        "train=crearFeatureHasher(train['text'],train)\n",
        "datos=crearFeatureHasher(datos['text'],datos)\n",
        "train.head()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKSNgTf-0HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['keyword'].fillna(value='ninguna',inplace=True)\n",
        "datos['keyword'].fillna(value='ninguna',inplace=True)\n",
        "train['keyword']=train['keyword'].str.casefold()\n",
        "datos['keyword']=datos['keyword'].str.casefold()\n",
        "def crearFeatureHasher(serie,ndf):\n",
        "  fh = FeatureHasher(n_features=6, input_type='string')\n",
        "  sp = fh.fit_transform(serie)\n",
        "  df = pd.DataFrame(sp.toarray(), columns=['kh1', 'kh2', 'kh3', 'kh4', 'kh5', 'kh6'])\n",
        "  ndf=pd.concat([ndf, df], axis=1)\n",
        "  return ndf\n",
        "train=crearFeatureHasher(train['keyword'],train)\n",
        "datos=crearFeatureHasher(datos['keyword'],datos)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrDzaLufqToa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creo un feature hash para el texto completo del tweet\n",
        "#Primero preproceso eliminando mayusculas, removiendo palabras comunes\n",
        "def reemplazarTexto(x):\n",
        "  x=x.casefold()\n",
        "  #x=x.replace(' the ',' ')\n",
        "  #x=x.replace(' a ',' ')\n",
        "  #x=x.replace(' is ',' ')\n",
        "  #x=x.replace(' are ',' ')\n",
        "  return x\n",
        "train['textoProcesado']=train['text'].apply(lambda x:reemplazarTexto(x))\n",
        "datos['textoProcesado']=datos['text'].apply(lambda x:reemplazarTexto(x))\n",
        "datos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw83Jm1uqStE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crearFeatureHasher(serie,ndf):\n",
        "  fh = FeatureHasher(n_features=21, input_type='string')\n",
        "  sp = fh.fit_transform(serie)\n",
        "  df = pd.DataFrame(sp.toarray(), columns=['fp1', 'fp2', 'fp3', 'fp4', 'fp5', 'fp6','fp7','fp8','fp9','fp10','fp11','fp12','fp13','fp14','fp15','fp16','fp17','fp18','fp19','fp20','fp21'])\n",
        "  ndf=pd.concat([ndf, df], axis=1)\n",
        "  return ndf\n",
        "train=crearFeatureHasher(train['textoProcesado'],train)\n",
        "datos=crearFeatureHasher(datos['textoProcesado'],datos)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAffCAe6Hfy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remuevo cosas que no hacen al analisis\n",
        "entrenar=train.drop(columns=['target','id','keyword','location','text','textoProcesado'])\n",
        "test=datos.drop(columns=['id','keyword','location','text','textoProcesado'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNqNHf0TsMQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=test.apply(lambda x:x/x.mean())\n",
        "entrenar=entrenar.apply(lambda x:x/x.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3h9Sv52Ihji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dY2-0wayohY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entrenar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG8QmUTRISuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "entrenar.info()\n",
        "#X_train, X_test, y_train, y_test =  train_test_split(entrenar, target, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7haa4xCWx4s9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n",
        "                colsample_bytree = 0.7, learning_rate = 0.08,\n",
        "                max_depth = 6, alpha = 10, n_estimators = 180)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovG65_arE-0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into X and y\n",
        "X = entrenar\n",
        "Y = target\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
        "# fit model no training data\n",
        "model = xg_reg\n",
        "eval_set = [(X_test, y_test)]\n",
        "model.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, verbose=True)\n",
        "# make predictions for test data\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZsQJ4u38nzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "model.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\n",
        "# make predictions for test data\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "# retrieve performance metrics\n",
        "results = model.evals_result()\n",
        "epochs = len(results['validation_0']['error'])\n",
        "x_axis = range(0, epochs)\n",
        "# plot log loss\n",
        "fig, ax = pyplot.subplots()\n",
        "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
        "ax.legend()\n",
        "pyplot.ylabel('Log Loss')\n",
        "pyplot.title('XGBoost Log Loss')\n",
        "pyplot.show()\n",
        "# plot classification error\n",
        "fig, ax = pyplot.subplots()\n",
        "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
        "ax.legend()\n",
        "pyplot.ylabel('Classification Error')\n",
        "pyplot.title('XGBoost Classification Error')\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5r-fjANksoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_encoded_y = LabelEncoder().fit_transform(target)\n",
        "model = XGBClassifier()\n",
        "colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
        "param_grid = dict(colsample_bylevel=colsample_bylevel)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
        "grid_result = grid_search.fit(X, label_encoded_y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "# plot\n",
        "pyplot.errorbar(colsample_bylevel, means, yerr=stds)\n",
        "pyplot.title(\"XGBoost colsample_bylevel vs Log Loss\")\n",
        "pyplot.xlabel('colsample_bylevel')\n",
        "pyplot.ylabel('Log Loss')\n",
        "pyplot.savefig('colsample_bylevel.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mJCGix9x-1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xg_reg.fit(entrenar,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmSoXon1FAo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = xg_reg.predict(test)\n",
        "preds = (preds>=0.5)*1\n",
        "predictions = pd.DataFrame()\n",
        "predictions['id']=datos['id']\n",
        "predictions['target']=preds\n",
        "predictions.head()\n",
        "predictions.to_csv('resultados.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6E0NLliIw1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!kaggle competitions submit -c nlp-getting-started -f resultados.csv -m \"Nuevo intento\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}